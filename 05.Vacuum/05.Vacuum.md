# ДЗ 05. Настройка autovacuum с учетом особеностей производительности (по лекции 08. MVCC, vacuum и autovacuum)

Создал виртуальную машину **otus-vm-05** в Yandex Cloud согласно [инструкции](../00.Common/01.YC_start.md). Версию PostgreSQL для установки выбираем **15**, как указано в задании. Также небольшое отличие от инструкции по ссылке в характеристиках созданной виртуальной машины: подключил SDD на 10ГБ вместо HDD в качестве основного диска виртуальной машины.

## 1. Создание тестовой БД

**1.1** В качестве тестовой БД будем использовать созданную при установке СУБД базу с именем `postgres`. Планируем использовать утилиту [`pgbench`](https://www.postgresql.org/docs/15/pgbench.html), с помощью которой будем проводить синтетические нагрузочные тесты на установленный кластер.

Создадим тестовые таблицы с предзаполненными данными:

```bash
# SSH-сессия на виртуальной машине
vm:~$ sudo -u postgres pgbench -i postgres
dropping old tables...
NOTICE:  table "pgbench_accounts" does not exist, skipping
NOTICE:  table "pgbench_branches" does not exist, skipping
NOTICE:  table "pgbench_history" does not exist, skipping
NOTICE:  table "pgbench_tellers" does not exist, skipping
creating tables...
generating data (client-side)...
100000 of 100000 tuples (100%) done (elapsed 0.09 s, remaining 0.00 s)
vacuuming...
creating primary keys...
done in 1.15 s (drop tables 0.00 s, create tables 0.01 s, client-side generate 0.12 s, vacuum 0.04 s, primary keys 0.99 s).
```

Здесь с использованием ключа `-i` указали имя БД `postgres`, где создаем тестовые таблицы.

**1.2** Запускаем утилиту для тестирования производительности со следующими параметрами

| Имя параметра | Значение |
|---------------|----------|
|-с8| Эмулируем 8 параллельно работающих клиентов |
|-P 6| Вывод информации о прогрессе каждые 6 секунд|
|-T 60| Выполнять нагрузку на протяжении 60 секунд||
|postgres| Имя БД, на которой необходимо выполнять нагрузку|

Получили следующие результаты:

```bash
# SSH-сессия на виртуальной машине
vm:~$ sudo -u postgres pgbench -c8 -P 6 -T 60 postgres
pgbench (15.6 (Ubuntu 15.6-1.pgdg20.04+1))
starting vacuum...end.
progress: 6.0 s, 655.0 tps, lat 12.132 ms stddev 8.643, 0 failed
progress: 12.0 s, 548.7 tps, lat 14.531 ms stddev 12.138, 0 failed
progress: 18.0 s, 585.5 tps, lat 13.648 ms stddev 10.463, 0 failed
progress: 24.0 s, 476.0 tps, lat 16.701 ms stddev 18.953, 0 failed
progress: 30.0 s, 307.2 tps, lat 26.130 ms stddev 27.742, 0 failed
progress: 36.0 s, 447.2 tps, lat 17.853 ms stddev 14.043, 0 failed
progress: 42.0 s, 527.0 tps, lat 15.145 ms stddev 11.685, 0 failed
progress: 48.0 s, 634.5 tps, lat 12.573 ms stddev 9.535, 0 failed
progress: 54.0 s, 501.5 tps, lat 15.832 ms stddev 16.392, 0 failed
progress: 60.0 s, 345.7 tps, lat 23.263 ms stddev 26.127, 0 failed
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 1
query mode: simple
number of clients: 8
number of threads: 1
maximum number of tries: 1
duration: 60 s
number of transactions actually processed: 30177
number of failed transactions: 0 (0.000%)
latency average = 15.873 ms
latency stddev = 15.849 ms
initial connection time = 19.705 ms
tps = 502.957067 (without initial connection time)
```

Т.е. среднее количество обработанных кластером транзакций в секунду составило **~500** транзакций со средней продолжительностью каждой из них **~16ms**.

**1.3** Редактируем файл **/etc/postgresql/15/main/postgresql.conf** и задаем там следующие настройки

|Имя параметра|Исходное значение|Новое значение|Комментарий|
|-|-|-|-|
|**max_connections** (максимальное число подключений к СУБД)|100|40|Учитывая, что мы планируем в нашем тесте эмулировать не более 8 подключений, то данная настройка не должна повлиять на производительность кластера в рамках наших испытаний|
|**shared_buffers** (размер условного "кеша" СУБД в памяти)|128MB|1GB|Как и предлагается в [документации](https://www.postgresql.org/docs/15/runtime-config-resource.html#GUC-SHARED-BUFFERS) выставляем значение в 25% от доступной памяти сервера |
|**effective_cache_size** (значение "оценки доступной памяти", которое будет использоваться для принятия решений о построении планов планировщиком запросов)|4GB|3GB|Насколько я понимаю, логика здесь выставить значение *<память сервера> - <размер shared_buffers>*, чтобы планировщик предполагал, что для выполнения запросов у него есть 3GB памяти и не приходилось уходить в swap при выполнении запросов. Однако не уверен, что в масштабах нашего синтетического теста это даст какой-либо эффект|
|**maintenance_work_mem** (максимальное количество памяти выделяемое для служебных операций типа VACUUM и команд DDL)|64МБ|512МБ|В нашем тесте будет активно использоваться автовакуум, потому, думаю, увеличение данного параметра позволит кластеру более полно использовать доступные ресурсы сервера|
|**checkpoint_completion_target** (целевое время завершения создания контрольной точки относительно времени до начала создания следующей контрольной точки. Позволяет контролировать нагрузку на диск)|0.9|0.9||
|**wal_buffers** (буфер в памяти для хранения журнала WAL)|-1 (что с измененным размером shared_buffers=1GB будет состалвять 1GB/32=32МБ)|16МБ|Сложно сказать почему в нашей ситуации оставить значение по умолчанию не будет верным решением|






default_statistics_target = 500 (было 100)
random_page_cost = 4 (так и было)
effective_io_concurrency = 2 (было 1)
work_mem = 6553kB (было 4MB)
min_wal_size = 4GB (было 80МБ)
max_wal_size = 16GB (был 1ГБ)

> [!NOTE]
> Я выполнял изменения напрямую в файле конфигурации **postgresql.conf**, однако возможно было также вынести только необходимые параметры в отдельный файл и воспользовальзоваться параметром `include_dir = <путь до папки с файлами конфигурации>` (см. https://www.postgresql.org/docs/15/config-setting.html), чтобы не редактировать каджый раз большой файл со всеми настройками.
