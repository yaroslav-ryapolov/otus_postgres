# Тема проекта: Сравнение производительности PostgreSQL в различных архитектурных решениях

## 1. Цели и задачи

Цель данной работы - на практике смоделировать ситуации недостатка различных ресурсов при нагрузке на кластер БД PostgreSQL и сравнить работу различных конфигураций кластера БД PostgreSQL под нагрузкой.

Более наглядный отчет представлен в [презентации](./Презентация%20проекта.%20Сравнение%20производительности%20конфигураций%20PostgreSQL.pdf).

Ниже постараюсь привести дополнительную (по отношению к презентации) информацию по настройке окружения и используемым инструментам.

## 2. Инструменты и настройка окружения

Все измерения в проекте производил на виртуальных машинах в [Yandex Cloud](https://yandex.cloud) с помощью утилиты [pgbench](https://www.postgresql.org/docs/current/pgbench.html).

Использовал **PostgreSQL 15**. В качестве кластера БД с шардированием использовал расширение для [Citus](https://www.citusdata.com/) для **PostgreSQL 15**.

Также использовал [pgbouncer](https://www.pgbouncer.org/) в качестве "пулера" подключений к БД.

Для тюнинга настроек PostgreSQL на каждом из узлов использовал онлайн конфигуратор [www.pgconfig.org](https://www.pgconfig.org) в соответствии с характеристиками виртуальной машины узла.

Для тестирования нагрузки на различных конфигурациях создал на Yandex Cloud 5 виртуальных машин:

|Название|CPU и RAM|Диски|Описание|
|-|-|-|-|
|1. project-loader (IP=192.168.0.5)|8 vCPU + 8ГБ RAM|20ГБ HDD|"нагрузчик", на котором запускал утилиту pgbench, чтобы не занимать ресурсы виртуальной машины кластера БД для чистоты измерений. Также сюда установил по тем же причинам "пулер соединений" pgbouncer для проведения измерений с его использованием|
|2. project-single-node (IP=192.168.0.23)|Настраивал в зависимости от целей измерения от 2 до 16 vCPU и от 4ГБ до 16ГБ RAM|20ГБ SSD для системы и 300ГБ HDD или высокопроизводительного SSD для каталога /var/lib/postgresql в зависимости от целей измерения|виртуальная машина для тестирования работы PostgreSQL в рамках установки на единую машину|
|3. project-citus-coord (IP=192.168.0.24)|Настраивал в зависимости от целей измерения от 2 до 4 vCPU + 4ГБ RAM|20ГБ SSD|узел-координатор для кластера Citus, который не будет хранить данные, но будет распределять нагрузку между шардами project-citus-1 и project-citus-2|
|идентичные машины 4. project-citus-1 (IP=192.168.0.11) и 5. project-citus-2 (IP=192.168.0.16)|2 vCPU + 4ГБ RAM|20ГБ SSD для системы и 300ГБ HDD или высокопроизводительного SSD для каталога /var/lib/postgresql в зависимости от целей измерения|узел-воркер для кластера Citus, который будет обрабатывать часть данных кластера|

> [!NOTE] Все установки PostgreSQL производил без дополнительных настроек под отказоустойчивость и восокопроизводительные диски даже использовал [нереплицируемые SSD](https://yandex.cloud/ru/docs/compute/concepts/disk), чтобы снизить стоимость виртуальных машин на время тестирования и обойти ограничения по квотам на реплицируемые высокопроизводительные диски для новых аккаунтов.

Наглядно взаимосвязь виртуальных машин представлена на схеме ниже ![схема узлов](./images/1.Nodes_schema.png "Схема узлов")

### 2.1 Установка БД на кластер с одной нодой

Согласно [инструкции](../00.Common/01.YC_start.md) создал виртуальную машину **project-single-node**. Версию PostgreSQL для установки выбираем **15**.

Подключал дополнительный диск для каталога `/var/lib/postgresql` в следующем порядке:

1. Разметил подключенный диск на один логический раздел командой `sudo fdisk /dev/vdb`
2. Отформатировал полученный раздел `sudo mkfs.ext4 /dev/vdb1`
3. Подмонтировал диск по временному пути и скопировал текущие данные в него командой `cp -rp /var/lib/postgresql/* /mnt/postgres-data`
4. Прописал подключение нового диска в `/etc/fstab` при загрузке
5. Перезагрузил машину

Для настройки под конкретные характеристики использовал скрипт из онлайн генератора [www.pgconfig.org](https://www.pgconfig.org)

Например, для 2 vCPU + 4ГБ RAM получил скрипт (`max_connections` сознательно выставлял в 200 для тестирования)

```sql
-- Generated by PGConfig 3.1.4 (1fe6d98dedcaad1d0a114617cfd08b4fed1d8a01)
-- https://api.pgconfig.org/v1/tuning/get-config?format=alter_system&include_pgbadger=true&log_format=csvlog&max_connections=100&pg_version=15&environment_name=WEB&total_ram=4GB&cpus=2&drive_type=SSD&arch=x86-64&os_type=linux

-- Memory Configuration
ALTER SYSTEM SET shared_buffers TO '1GB';
ALTER SYSTEM SET effective_cache_size TO '3GB';
ALTER SYSTEM SET work_mem TO '10MB';
ALTER SYSTEM SET maintenance_work_mem TO '205MB';

-- Checkpoint Related Configuration
ALTER SYSTEM SET min_wal_size TO '2GB';
ALTER SYSTEM SET max_wal_size TO '3GB';
ALTER SYSTEM SET checkpoint_completion_target TO '0.9';
ALTER SYSTEM SET wal_buffers TO '-1';

-- Network Related Configuration
ALTER SYSTEM SET listen_addresses TO '*';
ALTER SYSTEM SET max_connections TO '200';

-- Storage Configuration
ALTER SYSTEM SET random_page_cost TO '1.1';
ALTER SYSTEM SET effective_io_concurrency TO '200';

-- Worker Processes Configuration
ALTER SYSTEM SET max_worker_processes TO '8';
ALTER SYSTEM SET max_parallel_workers_per_gather TO '2';
ALTER SYSTEM SET max_parallel_workers TO '2';

-- Logging configuration for pgbadger
ALTER SYSTEM SET logging_collector TO 'on';
ALTER SYSTEM SET log_checkpoints TO 'on';
ALTER SYSTEM SET log_connections TO 'on';
ALTER SYSTEM SET log_disconnections TO 'on';
ALTER SYSTEM SET log_lock_waits TO 'on';
ALTER SYSTEM SET log_temp_files TO '0';
ALTER SYSTEM SET lc_messages TO 'C';

-- Adjust the minimum time to collect the data
ALTER SYSTEM SET log_min_duration_statement TO '10s';
ALTER SYSTEM SET log_autovacuum_min_duration TO '0';

-- CSV Configuration
ALTER SYSTEM SET log_destination TO 'csvlog';
```

Для конфигурации 8 vCPU + 16ГБ RAM (`max_connections` сознательно выставлял в 200 для тестирования)

```sql
-- Memory Configuration
ALTER SYSTEM SET shared_buffers TO '4GB';
ALTER SYSTEM SET effective_cache_size TO '12GB';
ALTER SYSTEM SET work_mem TO '41MB';
ALTER SYSTEM SET maintenance_work_mem TO '819MB';

-- Checkpoint Related Configuration
ALTER SYSTEM SET min_wal_size TO '2GB';
ALTER SYSTEM SET max_wal_size TO '3GB';
ALTER SYSTEM SET checkpoint_completion_target TO '0.9';
ALTER SYSTEM SET wal_buffers TO '-1';

-- Network Related Configuration
ALTER SYSTEM SET listen_addresses TO '*';
ALTER SYSTEM SET max_connections TO '200';

-- Storage Configuration
ALTER SYSTEM SET random_page_cost TO '1.1';
ALTER SYSTEM SET effective_io_concurrency TO '200';

-- Worker Processes Configuration
ALTER SYSTEM SET max_worker_processes TO '8';
ALTER SYSTEM SET max_parallel_workers_per_gather TO '2';
ALTER SYSTEM SET max_parallel_workers TO '2';

-- Logging configuration for pgbadger
ALTER SYSTEM SET logging_collector TO 'on';
ALTER SYSTEM SET log_checkpoints TO 'on';
ALTER SYSTEM SET log_connections TO 'on';
ALTER SYSTEM SET log_disconnections TO 'on';
ALTER SYSTEM SET log_lock_waits TO 'on';
ALTER SYSTEM SET log_temp_files TO '0';
ALTER SYSTEM SET lc_messages TO 'C';

-- Adjust the minimum time to collect the data
ALTER SYSTEM SET log_min_duration_statement TO '10s';
ALTER SYSTEM SET log_autovacuum_min_duration TO '0';

-- CSV Configuration
ALTER SYSTEM SET log_destination TO 'csvlog';
```

С помощью утилиты pgbench (запускал на машине project-loader) сгенерировал 4 БД различного размера под стандартный скрипт командами

|Имя БД|Записей в pgbench_accounts|Команда для генерации|
|-|-|-|
|pgbench_1kk|1 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.23 -U postgres -i -s 10 pgbench_1kk`|
|pgbench_10kk|10 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.23 -U postgres -i -s 100 pgbench_10kk`|
|pgbench_100kk|100 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.23 -U postgres -i -s 1000 pgbench_100kk`|
|pgbench_1kkk|1 000 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.23 -U postgres -i -s 10000 pgbench_1kkk`|

В сумме все наполненные БД с индексами заняли около 190ГБ места на диске.

### 2.2 Установка кластера Citus из 3 узлов

Citus устанавливал на каждый узел согласно инструкции [Multi-Node Citus on Debian or Ubuntu](https://docs.citusdata.com/en/v12.1/installation/multi_node_debian.html).

Отмечу только, что не стоит забывать, что Citus - это extension для PostgreSQL, потому все настройки необходимо делать явно для каждой БД, в которой планируется использовать его возможности.

В планах было шардировать все таблицы по полю bid (идентификатор для pgbench_branches). Потому пришлось несколько скорректировать исходный скрипт для создания БД и дополнить его созданием уникальных ключей с bid во всех таблицах (одно из требований для шардирования) и добавить уникальный автогенерируемый столбец-идентификатор к таблице pgbench_history. Получился следующий скрипт

```sql
create table if not exists pgbench_history  
(  
    hid    integer not null generated always as identity,
    tid    integer,  
    bid    integer,  
    aid    integer,  
    delta  integer,  
    mtime  timestamp,  
    filler char(22),

    PRIMARY KEY (bid, hid)
);  
  
alter table pgbench_history  
    owner to postgres;  
  
create table if not exists pgbench_tellers  
(  
    tid      integer not null,  
    bid      integer,  
    tbalance integer,  
    filler   char(84),
    
    PRIMARY KEY (bid, tid)
)  
    with (fillfactor = 100);  
  
alter table pgbench_tellers  
    owner to postgres;  
  
create table if not exists pgbench_accounts  
(  
    aid      integer not null,  
    bid      integer,  
    abalance integer,  
    filler   char(84),

    PRIMARY KEY (bid, aid)
)  
    with (fillfactor = 100);  
  
alter table pgbench_accounts  
    owner to postgres;  
  
create table if not exists pgbench_branches  
(  
    bid      integer not null  
        primary key,  
    bbalance integer,  
    filler   char(88)
)  
    with (fillfactor = 100);  
  
alter table pgbench_branches  
    owner to postgres;

SELECT create_distributed_table('pgbench_history',  'bid');
SELECT create_distributed_table('pgbench_tellers',  'bid');
SELECT create_distributed_table('pgbench_accounts', 'bid');
SELECT create_distributed_table('pgbench_branches', 'bid');
```

Как и в пункте 2.2 создал 4 БД с разным количеством данных (наполнял так же утилитой pgbench с пропуском стадии генерации таблиц)

|Имя БД|Записей в pgbench_accounts|Команда для генерации данных|
|-|-|-|
|pgbench_1kk|1 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.24 -U postgres -i -I g -s 10 pgbench_1kk`|
|pgbench_10kk|10 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.24 -U postgres -i -I g -s 100 pgbench_10kk`|
|pgbench_100kk|100 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.24 -U postgres -i -I g -s 1000 pgbench_100kk`|
|pgbench_1kkk|1 000 000 000|`export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.24 -U postgres -i -I g -s 10000 pgbench_1kkk`|

Для нагрузки pgbench так же была необходимость изменить встроенный скрипт и дополнить его данными о bid в каждом запросе (чтобы Citus мог сразу определить запрос на конкретный узел)

```sql
\set bid random(1, 1 * :scale)
\set aid ((:bid - 1) * 100000 + random(1, 100000))
\set tid random(10 * :bid - 9, 10 * :bid)
\set delta random(-5000, 5000)
BEGIN;
UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid AND bid = :bid;
SELECT abalance FROM pgbench_accounts WHERE aid = :aid AND bid = :bid;
UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid AND bid = :bid;
UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid;
INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP);
END;
```

В итоге запускал нагрузку командой вида (важно отметить, что т.к. скрипт кастомный, то была необходимость явно передавать ключ `-s <Scale Factor>` при каждом запуске)

```bash
export PGPASSWORD=TheStrongPassword123 && pgbench -h 192.168.0.24 -U postgres -j 8 -c 32 -T 300 -P 30 pgbench_1kkk -f pgbench_citus.sql -s 10000
```

### 2.3 Установка ПО на узел для запуска pbgench

Для узла project-loader cогласно [инструкции](../00.Common/01.YC_start.md) создал виртуальную машину **project-lader** с версией PostgreSQL для установки **15**.

Также установил на этот узел пулер соединений pgbouncer согласно инструкии [pgbouncer tutorial](https://www.scaleway.com/en/docs/tutorials/install-pgbouncer/) с примерными шагами

**1. Установка пакета.**

```bash
sudo  apt install pgbouncer -y
```

**2. Правка конфигурации.**

Подправил файл `/etc/pgbouncer/pgbouncer.ini` и выставил максимальное количество подключений к БД в 24 (это число вывел эмпирически при сравнении результатов tps под нагрузкой заданного характера TPC-B)

```ini
[databases]
* = host=192.168.0.23 port=5432 pool_mode=transaction max_db_connections=24

[pgbouncer]
;; any, trust, plain, md5, cert, hba, pam
auth_type = hba
auth_file = /etc/pgbouncer/userlist.txt

;; Path to HBA-style auth config
auth_hba_file = /etc/pgbouncer/pb_hba.conf
```

Создал файл `/etc/pgbouncer/pb_hba.conf` с возможностью подключения локально

```config
# TYPE DATABASE USER ADDRESS METHOD
# IPv4 local connections:
host all all 127.0.0.1/32 scram-sha-256
# IPv6 local connections:
host all all ::1/128 scram-sha-256
# Application
# host all all APPLICATION_IP/NETMASK scram-sha-256
```

Подправил `/etc/pgbouncer/userlist.txt` (пароль взял из БД на узле project-single-node через `SELECT usename,passwd FROM pg_shadow;`)

```config
"postgres" "SCRAM-SHA-256$4096:1k9vb8kGpmkaCMSemXrpcw==$ux8+/lS24FiV2UvOnslnTfGM7vr298IG9uWeFEQUnqo=:i4JCgrtz4lJIDhy4+a7glxFCaStHub8pYfsQyP58/Hk="
```

**3. Перезапуск сервиса pgbouncer.**

```bash
sudo systemctl reload pgbouncer.service
```

В итоге при подключении под адресу localhost:6432 была возможность подключаться через pgbouncer к узлу **project-single-node** и давать нагрузку туда командой вида

```bash
 export PGPASSWORD=TheStrongPassword123 && pgbench -h localhost -p 6432 -U postgres -j 8 -c 96 -T 300 -P 30 pgbench_1kkk
```

## 3. Результаты измерений

Сводная информация есть в разделе презентации. Картинки с мониторинга и вывод команд pgbench находится ...в процессе оформления...
